{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip\n",
    "import imageio_ffmpeg as ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['IMAGEMAGICK_BINARY'] = r'C:\\Program Files\\ImageMagick-7.0.11-Q16-HDRI\\convert.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\AT\\Downloads\\MovieData\\video_0000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract frames from video\n",
    "def extract_frames(video_path, num_frames=16):\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    interval = max(total_frames // num_frames, 1)\n",
    "    for i in range(0, total_frames, interval):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(cv2.resize(frame, (224, 224)))\n",
    "        if len(frames) == num_frames:\n",
    "            break\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate captions\n",
    "def generate_caption(model, feature_extractor, tokenizer, frames):\n",
    "    inputs = feature_extractor(images=frames, return_tensors=\"pt\")\n",
    "    pixel_values = inputs.pixel_values\n",
    "    output_ids = model.generate(pixel_values)\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate caption for a specific video\n",
    "def generate_caption_for_video(video_path, model, feature_extractor, tokenizer):\n",
    "    frames = extract_frames(video_path)\n",
    "    caption = generate_caption(model, feature_extractor, tokenizer, frames)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add text descriptions to video\n",
    "def add_text_descriptions_to_video(video_path, description, output_path):\n",
    "    # Load the video\n",
    "    video = VideoFileClip(video_path)\n",
    "    \n",
    "    # Debug: Print the duration of the video\n",
    "    print(f\"Video Duration: {video.duration}\")\n",
    "    \n",
    "    # Ensure duration is correctly set\n",
    "    duration = video.duration\n",
    "    \n",
    "    if duration is None:\n",
    "        raise ValueError(\"The duration of the video clip is None. Check the video file.\")\n",
    "\n",
    "    # Create a TextClip object with duration set\n",
    "    text_clip = TextClip(description, fontsize=24, color='white', bg_color='black', size=(video.size[0], 50))\n",
    "    text_clip = text_clip.set_position(('center', 'bottom')).set_duration(duration)\n",
    "    \n",
    "    # Debug: Print the duration of the text clip\n",
    "    print(f\"TextClip Duration: {text_clip.duration}\")\n",
    "    \n",
    "    # Composite the video with the text overlay\n",
    "    result = CompositeVideoClip([video, text_clip])\n",
    "    \n",
    "    # Write the result to the output file\n",
    "    result.write_videofile(output_path, codec='libx264', audio_codec='aac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a traffic light with a street sign on it \n"
     ]
    }
   ],
   "source": [
    "video_path = r'C:\\Users\\AT\\Downloads\\MovieData\\video_0000\\1007770414.mp4'  # Specify the path to the video\n",
    "description = generate_caption_for_video(video_path, model, feature_extractor, tokenizer)\n",
    "print(f\"Generated Caption: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Duration: 9.27\n",
      "TextClip Duration: 9.27\n",
      "Moviepy - Building video C:\\Users\\AT\\Downloads\\MovieData\\video_0000\\1007770414_withalign.mp4.\n",
      "Moviepy - Writing video C:\\Users\\AT\\Downloads\\MovieData\\video_0000\\1007770414_withalign.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\AT\\Downloads\\MovieData\\video_0000\\1007770414_withalign.mp4\n"
     ]
    }
   ],
   "source": [
    "output_path = r'C:\\Users\\AT\\Downloads\\MovieData\\video_0000\\1007770414_withalign.mp4'\n",
    "add_text_descriptions_to_video(video_path, description, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
